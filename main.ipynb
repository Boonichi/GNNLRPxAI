{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import scispacy\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(37)\n",
    "random.seed(37)\n",
    "w2v_file = \"./data/embedding/wiki-news-300d-1M.vec\"\n",
    "dir_vocab = \"./data/embedding/\"\n",
    "train_dir = \"./data/PubMed_20k_RCT/train.txt\"\n",
    "test_dir = \"./data/PubMed_20k_RCT/test.txt\"\n",
    "dev_dir = \"./data/PubMed_20k_RCT/dev.txt\"\n",
    "language_model = \"en_core_sci_sm\"\n",
    "vocab_dir = \"./data/embedding/vocab_size_50000_min_-11.0164_max_2.3578.p\"\n",
    "explain_dir = \"./data/explanations/explanations.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Prepare fasttext\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "fasttext = KeyedVectors.load_word2vec_format(w2v_file, limit = VOCAB_SIZE)\n",
    "\n",
    "word2vec = {}\n",
    "\n",
    "lower_bound = float('inf')\n",
    "upper_bound = float('-inf')\n",
    "\n",
    "sum_of_vectors = None\n",
    "for word in fasttext.index_to_key:\n",
    "    word2vec[word] = np.reshape(fasttext[word], (1, - 1))\n",
    "    \n",
    "    min_coeff = np.min(word2vec[word])\n",
    "    max_coeff = np.max(word2vec[word])\n",
    "    lower_bound = min_coeff if min_coeff < lower_bound else lower_bound\n",
    "    upper_bound = max_coeff if max_coeff > upper_bound else upper_bound\n",
    "\n",
    "    if sum_of_vectors is not None:\n",
    "        sum_of_vectors = sum_of_vectors +  word2vec[word]\n",
    "    else:\n",
    "        sum_of_vectors = word2vec[word]\n",
    "\n",
    "sum_of_vectors /= VOCAB_SIZE\n",
    "unk = \"<###-unk-###>\"\n",
    "word2vec[unk] = sum_of_vectors\n",
    "\n",
    "max_coeff = np.max(word2vec[unk])\n",
    "upper_bound = max_coeff if max_coeff > upper_bound else upper_bound\n",
    "min_coeff = np.min(word2vec[unk])\n",
    "lower_bound = lower_bound if min_coeff < lower_bound else lower_bound\n",
    "\n",
    "lower_bound = str(round(lower_bound, ndigits=5))\n",
    "upper_bound = str(round(upper_bound, ndigits=5))\n",
    "\n",
    "file_name = f'vocab_size_{VOCAB_SIZE}_min_{lower_bound}_max_{upper_bound}.p'\n",
    "path = os.path.join(dir_vocab, file_name)\n",
    "pickle.dump(word2vec, open(path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgcn.xgraph import XNode, XGraph\n",
    "\n",
    "def doc2graph(doc,to_lower):\n",
    "    def add(graph, token, to_lower=to_lower):\n",
    "        id = token.i + 1 \n",
    "      \n",
    "        if graph.contains_by_id(id):\n",
    "            raise AssertionError('Node contained.')\n",
    "        label = token.text \n",
    "        if to_lower:\n",
    "            label = label.lower()\n",
    "        n = XNode(id=id, label=label, type='TOKEN')\n",
    "        graph.add_node(n)\n",
    "        return n\n",
    "\n",
    "    graph = XGraph()\n",
    "\n",
    "    for idx in range(len(doc)):\n",
    "        token = doc[idx]\n",
    "        add(graph, token)\n",
    "\n",
    "    for idx in range(len(doc)):\n",
    "        parent_token = doc[idx]\n",
    "        parent_id = parent_token.i + 1  \n",
    "        parent_node = graph.get_node(parent_id)\n",
    "        for child_token in parent_token.children:\n",
    "            child_id = child_token.i + 1  \n",
    "            child_node = graph.get_node(child_id)\n",
    "            graph.add_edge(parent_node, child_node, t=child_token.dep_)\n",
    "\n",
    "    return graph\n",
    "    \n",
    "def line_to_graph(line, nlp):\n",
    "    label, sent = line.split('\\t')[0], line.split('\\t')[1]\n",
    "    sent = sent.strip()\n",
    "    doc = nlp(sent)\n",
    "    print(sent)\n",
    "    print(doc)\n",
    "    g = doc2graph(doc=doc, to_lower=True)\n",
    "    return label, g\n",
    "def preprocess(path, limit = 100):\n",
    "    pattern = \"###[0-9]+$\"\n",
    "    pattern = re.compile(pattern)\n",
    "    \n",
    "    path_out = path.replace('.txt', '.p')\n",
    "    \n",
    "    f_in = open(path, 'r')\n",
    "    lines = f_in.readlines()\n",
    "    graphs = []\n",
    "    \n",
    "    nlp = spacy.load(language_model, disable = ['tagger',\n",
    "                                              'ner',\n",
    "                                              'textcat',\n",
    "                                              'entity_ruler',\n",
    "                                              'sentenizer',\n",
    "                                              'merge_noun_chunks',\n",
    "                                              'merge_entities',\n",
    "                                              'merge_subtokens'])\n",
    "    \n",
    "    written = 0\n",
    "    discarded = 0\n",
    "    lines = random.sample(lines, int(limit / 100 * len(lines)))\n",
    "    for line in tqdm(lines):\n",
    "        line = line.strip()\n",
    "        if len(line) == 0 or pattern.match(line.strip()):\n",
    "            discarded = discarded + 1\n",
    "            continue\n",
    "        label, graph = line_to_graph(line.strip(), nlp)\n",
    "        graphs.append((label,graph))\n",
    "        written = written + 1\n",
    "        if (written % 1000 == 999):\n",
    "            print('Processed {} lines and discarded {} lines'.format(written + 1, discarded + 1))\n",
    "    \n",
    "    f_in.close()\n",
    "\n",
    "    pickle.dump(graphs, open(path_out, 'wb'))\n",
    "    return path_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84016 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "  0%|          | 0/84016 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blood samples were drawn at the beginning ( t0 ) and end ( t1 ) of the operation and after 24h ( t2 ) .\n",
      "Blood samples were drawn at the beginning ( t0 ) and end ( t1 ) of the operation and after 24h ( t2 ) .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35135 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cumulative number of new gadolinium-enhancing T1 lesions was reduced by 67.9 % compared to placebo ( p = 0.002 ) .\n",
      "The cumulative number of new gadolinium-enhancing T1 lesions was reduced by 67.9 % compared to placebo ( p = 0.002 ) .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35212 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant changes were noted in the other two groups .\n",
      "No significant changes were noted in the other two groups .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/PubMed_20k_RCT/dev.p'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing in PubMed Dataset\n",
    "preprocess(path = train_dir, limit = 40)\n",
    "preprocess(path = test_dir)\n",
    "preprocess(path = dev_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from xgcn.xgcn import XGCN\n",
    "from xgcn.xgraph import XSample, Pad, ToTensor, LabelToOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader\n",
    "\n",
    "class PubMedLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, path_pickle, w2v_path, pad = 150, crop =-1):\n",
    "        self.path_word2vec = w2v_path\n",
    "        self.label2vec = pickle.load(open(w2v_path, 'rb'))\n",
    "        self.path_pickle = path_pickle\n",
    "        self.label_graph_tuples = pickle.load(open(self.path_pickle, 'rb'))\n",
    "        self.label2onehot = LabelToOneHot(classes = PubMedLoader.classes())\n",
    "        self.crop = crop\n",
    "        self.pad = pad\n",
    "        self.ops = [Pad(self.pad), ToTensor(), self.label2onehot]\n",
    "        self.transforms = transforms.Compose(self.ops)\n",
    "    \n",
    "    @staticmethod\n",
    "    def classes():\n",
    "        return [\"METHODS\", \"RESULTS\", \"CONCLUSIONS\", \"BACKGROUND\", \"OBJECTIVE\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.crop > 0:\n",
    "            return self.crop\n",
    "        return len(self.label_graph_tuples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label, graph = self.label_graph_tuples[index]\n",
    "        embedding = graph.E(label2vec = self.label2vec)\n",
    "        adjacency = graph.A_tilde()\n",
    "        \n",
    "        xsample = XSample(embedding, adjacency, label)\n",
    "        xsample = self.transforms(xsample)\n",
    "        return xsample.EMBEDDING, xsample.ADJACENCY, xsample.LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(train_path, dev_path, test_path, w2v_path, pad, crop_train, crop_dev, crop_test, batch_size, num_workers):\n",
    "    pin_memory = torch.cuda.is_available()\n",
    "    \n",
    "    train_dataset = PubMedLoader(path_pickle = train_dir.replace(\".txt\", \".p\"), w2v_path = w2v_path, pad = pad, crop = crop_train)\n",
    "    dev_dataset = PubMedLoader(path_pickle = dev_dir.replace(\".txt\",\".p\"), w2v_path = w2v_path, pad = pad, crop = crop_dev)\n",
    "    test_dataset = PubMedLoader(path_pickle = test_dir.replace(\".txt\",\".p\"), w2v_path = w2v_path, pad = pad, crop = crop_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size ,pin_memory=pin_memory, num_workers=num_workers)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size = batch_size ,pin_memory=pin_memory, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size ,pin_memory=pin_memory, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, dev_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(epoch, split, scores):\n",
    "    print(\"Epoch: {} Split: {} F-micro: {:.3f} F-macro: {:.3f} F-weighted: {:.3f}\"\n",
    "        .format(epoch, split, scores['micro'], scores['macro'], scores['weighted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(xgcn, dataloader,device):\n",
    "    xgcn.eval()\n",
    "    xgcn.to(device)\n",
    "    outputs = None\n",
    "    targets = None\n",
    "    for (embeddings, adjacencies, labels) in tqdm(dataloader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        adjacencies = adjacencies.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if targets is None:\n",
    "            targets = labels\n",
    "        else:\n",
    "            targets = torch.cat((targets, labels))\n",
    "\n",
    "        output = xgcn(embeddings, adjacencies)\n",
    "        output = torch.argmax(output, dim=1)\n",
    "\n",
    "        if outputs is None:\n",
    "            outputs = output\n",
    "        else:\n",
    "            outputs = torch.cat((outputs, output))\n",
    "\n",
    "    outputs = outputs.tolist()\n",
    "    targets = targets.tolist()\n",
    "\n",
    "    outputs, targets = zip(*((output, target) for output, target in zip(outputs, targets))) # todo what does this do?\n",
    "    outputs = list(outputs)\n",
    "    targets = list(targets)\n",
    "\n",
    "    f_score_micro = f1_score(y_pred=outputs, y_true=targets, average='micro')\n",
    "    f_score_macro = f1_score(y_pred=outputs, y_true=targets, average='macro')\n",
    "    f_score_weighted = f1_score(y_pred=outputs, y_true=targets, average='weighted')\n",
    "    logging.info('...done validating.')\n",
    "\n",
    "    return {'micro': f_score_micro,\n",
    "            'macro': f_score_macro,\n",
    "            'weighted': f_score_weighted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,dev_loader, path_model,epochs, batch_size, pad, nfeat, nhid, patience, metric, random_seed,nclasses):\n",
    "    print(train_loader.__len__())\n",
    "    device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    xgcn = XGCN(nfeat = nfeat, nhid = nhid, nclass = nclasses, pad= pad, bias = None)\n",
    "    \n",
    "    xgcn.to(device)\n",
    "    optimizer = Adam(params = xgcn.parameters())\n",
    "    \n",
    "    #xgcn.load_state_dict(torch.load(\"model.weights\"))\n",
    "    scores = validate(xgcn = xgcn, dataloader = dev_loader, device = device)\n",
    "    \n",
    "    report(epoch = 0, split = \"Dev\", scores = scores)\n",
    "    torch.save(xgcn.state_dict(), path_model)\n",
    "    print(\"Saved initial model to {}.\".format(path_model))\n",
    "    \n",
    "    wait = 0\n",
    "    score_last = float('-inf')\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        xgcn.train()\n",
    "        for batch_idx, (embeddings, adjacencies, labels) in enumerate(train_loader):\n",
    "            embeddings = embeddings.to(device)\n",
    "            adjacencies = adjacencies.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = xgcn(embeddings,adjacencies)\n",
    "            loss = F.nll_loss(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            xgcn.xfc.weight.data.clamp_(0)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 10 == 9:\n",
    "                print('[%d, %5d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, batch_idx + 1, (batch_idx + 1) * batch_size, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        scores = validate(xgcn = xgcn, dataloader = dev_loader, device = device)\n",
    "        report(epoch = epoch + 1, split = \"Dev\", scores = scores)\n",
    "        \n",
    "        score_current = scores[metric]\n",
    "        \n",
    "        if score_current > score_last:\n",
    "            torch.save(xgcn.state_dict(), path_model)\n",
    "            print(\"{} score improved from {:.3f} to {:.3f}. Saved model to {}.\"\n",
    "                .format(metric, score_last, score_current, path_model))\n",
    "            score_last = score_current\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait = wait + 1\n",
    "            if wait >= patience:\n",
    "                print(\"Terminating training after {} epochs w/o improvement.\".format(wait))\n",
    "                return xgcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Train Process\n",
    "pad = 350\n",
    "crop_train = -1\n",
    "crop_dev = -1\n",
    "crop_test = -1\n",
    "batch_size = 8\n",
    "num_workers = 64\n",
    "epochs = 100\n",
    "nclasses = 5\n",
    "path_model = \"./data/model/model.weights\"\n",
    "nfeat = 300\n",
    "nhid = 300\n",
    "patience = 3\n",
    "metric = \"weighted\"\n",
    "random_seed = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, dev_loader, test_loader = load(train_path=train_dir,dev_path =  dev_dir,test_path = test_dir, w2v_path = vocab_dir, pad = pad, crop_train = crop_train, crop_dev = crop_dev, crop_test = crop_test, batch_size = batch_size, num_workers = num_workers)\n",
    "\n",
    "print(\"Start dump loader\")\n",
    "#pickle.dump(train_loader, open(\"train_loader.p\", 'wb'))\n",
    "#pickle.dump(dev_loader, open(\"dev_loader.p\", 'wb'))\n",
    "#pickle.dump(test_loader, open(\"test_loader.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3777 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'PubMedLoader' on <module '__main__' (built-in)>\n",
      "  0%|          | 0/3777 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xgcn \u001b[39m=\u001b[39m train(train_loader, dev_loader,path_model, epochs, batch_size, pad, nfeat, nhid,patience, metric, random_seed, nclasses \u001b[39m=\u001b[39;49m nclasses)\n",
      "Cell \u001b[0;32mIn [13], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, dev_loader, path_model, epochs, batch_size, pad, nfeat, nhid, patience, metric, random_seed, nclasses)\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(params \u001b[39m=\u001b[39m xgcn\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m     10\u001b[0m \u001b[39m#xgcn.load_state_dict(torch.load(\"model.weights\"))\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m scores \u001b[39m=\u001b[39m validate(xgcn \u001b[39m=\u001b[39;49m xgcn, dataloader \u001b[39m=\u001b[39;49m dev_loader, device \u001b[39m=\u001b[39;49m device)\n\u001b[1;32m     13\u001b[0m report(epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, split \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDev\u001b[39m\u001b[39m\"\u001b[39m, scores \u001b[39m=\u001b[39m scores)\n\u001b[1;32m     14\u001b[0m torch\u001b[39m.\u001b[39msave(xgcn\u001b[39m.\u001b[39mstate_dict(), path_model)\n",
      "Cell \u001b[0;32mIn [7], line 6\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(xgcn, dataloader, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      5\u001b[0m targets \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m (embeddings, adjacencies, labels) \u001b[39min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m      7\u001b[0m     embeddings \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     adjacencies \u001b[39m=\u001b[39m adjacencies\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1027\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgcn = train(train_loader, dev_loader,path_model, epochs, batch_size, pad, nfeat, nhid,patience, metric, random_seed, nclasses = nclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "xgcn = XGCN(nfeat = nfeat, nhid = nhid, nclass = nclasses, pad= pad, bias = None)\n",
    "xgcn.load_state_dict(torch.load(\"model.weights\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "scores = validate(xgcn = xgcn, dataloader = test_loader, device = device)\n",
    "report(0, split = \"Test\", scores = scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35135 [00:00<?, ?it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 4.76837158203125e-07\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      "  0%|          | 4/35135 [00:00<1:54:15,  5.12it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 9.5367431640625e-07\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      "  0%|          | 21/35135 [00:03<1:57:26,  4.98it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 2.384185791015625e-07\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      "  0%|          | 47/35135 [00:08<1:58:52,  4.92it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 2.86102294921875e-06\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      "  0%|          | 90/35135 [00:16<1:56:21,  5.02it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 1.9073486328125e-06\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      "  0%|          | 114/35135 [00:20<1:41:38,  5.74it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 1.430511474609375e-06\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      "  1%|          | 192/35135 [00:35<2:04:22,  4.68it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 1.1920928955078125e-07\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      "  2%|▏         | 674/35135 [02:05<1:55:08,  4.99it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 2.384185791015625e-06\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      "  2%|▏         | 839/35135 [02:37<2:00:09,  4.76it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 3.814697265625e-06\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      "  8%|▊         | 2835/35135 [08:52<1:54:14,  4.71it/s]/Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 7.152557373046875e-07\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      " 43%|████▎     | 15052/35135 [1:00:03<1:12:12,  4.64it/s] /Users/phanvanhung/GNNLRP/xgcn/xgcn.py:404: UserWarning: Conservation property violated with a difference of 5.960464477539063e-08\n",
      "  warnings.warn('Conservation property violated with a difference of {}'.format(diff))\n",
      " 52%|█████▏    | 18106/35135 [1:09:44<1:00:46,  4.67it/s]/Users/phanvanhung/GNNLRP/explain.py:231: RuntimeWarning: invalid value encountered in divide\n",
      "  global_normalized_relevance_matrix = (relevance_matrices[0] + relevance_matrices[1]) / np.sum(\n",
      "/Users/phanvanhung/GNNLRP/explain.py:234: UserWarning: After normalization sum of weights not close to 1 in line 15544.\n",
      "  warnings.warn(f\"After normalization sum of weights not close to 1 in line {line_counter}.\")\n",
      " 58%|█████▊    | 20401/35135 [1:17:06<52:08,  4.71it/s]  /Users/phanvanhung/GNNLRP/explain.py:234: UserWarning: After normalization sum of weights not close to 1 in line 17519.\n",
      "  warnings.warn(f\"After normalization sum of weights not close to 1 in line {line_counter}.\")\n",
      " 99%|█████████▉| 34903/35135 [2:03:45<00:44,  5.24it/s]  /Users/phanvanhung/GNNLRP/explain.py:234: UserWarning: After normalization sum of weights not close to 1 in line 29937.\n",
      "  warnings.warn(f\"After normalization sum of weights not close to 1 in line {line_counter}.\")\n",
      "100%|██████████| 35135/35135 [2:04:30<00:00,  4.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explain\n",
    "from explain import *\n",
    "explain(nfeat=nfeat,\n",
    "            nhid=nhid,\n",
    "            path_model=path_model,\n",
    "            padding=pad,\n",
    "            path_text=test_dir,\n",
    "            path_out=\"./data/explanations/explanations.jsonl\",\n",
    "            path_label2vec=vocab_dir,\n",
    "            lower_bound=float(lower_bound),\n",
    "            upper_bound=float(upper_bound),\n",
    "            to_lower=True,\n",
    "            language_model=language_model,\n",
    "            crop=-1,\n",
    "            do_occlude= True,\n",
    "            drop=1.0,\n",
    "            step=0.1,                           \n",
    "            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing occlusion experiments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30135it [13:35, 36.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to latex...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/explanations/explanations.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     fout\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mConverting to latex...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m to_latex(path_in\u001b[39m=\u001b[39;49mexplain_dir,\n\u001b[1;32m     26\u001b[0m             path_out\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./data/explanations/explanations.tex\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m             max_seq_len\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m             crop\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m             weight\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m             base\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m...done converting to latex.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/GNNLRP/postprocess.py:150\u001b[0m, in \u001b[0;36mto_latex\u001b[0;34m(path_in, path_out, weight, base, max_seq_len, crop)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_latex\u001b[39m(path_in, path_out, weight, base, max_seq_len\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, crop\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    149\u001b[0m     all_explanations \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 150\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path_in, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m    151\u001b[0m         \u001b[39mfor\u001b[39;00m jsonl \u001b[39min\u001b[39;00m tqdm(fin):\n\u001b[1;32m    152\u001b[0m             \u001b[39m# early stopping (consider tex out-of-resources error)\u001b[39;00m\n\u001b[1;32m    153\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m crop \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(all_explanations):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/explanations/explanations.jsonl'"
     ]
    }
   ],
   "source": [
    "#postprocess\n",
    "from postprocess import *\n",
    "\n",
    "print('Summarizing occlusion experiments...')\n",
    "top, bottom = read_explanations(explain_dir)\n",
    "res_top, percentages = occlusion_predictions(top)\n",
    "res_bottom, percentages = occlusion_predictions(bottom)\n",
    "f1_top = [f1_score(t[0], t[1], average='weighted') for t in res_top]\n",
    "\n",
    "f1_top = list(zip(percentages, f1_top))\n",
    "f1_top = [f'{tup[0]},{tup[1]}' for tup in f1_top]\n",
    "f1_top = '\\n'.join(f1_top)\n",
    "f1_bottom = [f1_score(b[0], b[1], average='weighted') for b in res_bottom]\n",
    "f1_bottom = list(zip(percentages, f1_bottom))\n",
    "f1_bottom = [f'{tup[0]},{tup[1]}' for tup in f1_bottom]\n",
    "f1_bottom = '\\n'.join(f1_bottom)\n",
    "with open(\"./data/top_masked_predictions.csv\", 'w+') as fout:\n",
    "    fout.write(f1_top)\n",
    "    fout.close()\n",
    "with open(\"./data/bottom_masked_predictions.csv\", 'w+') as fout:\n",
    "    fout.write(f1_bottom)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to latex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1110it [00:25, 43.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...done converting to latex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Converting to latex...')\n",
    "to_latex(path_in=explain_dir,\n",
    "            path_out=\"./data/explanations/explanations.tex\",\n",
    "            max_seq_len=10,\n",
    "            crop=250,\n",
    "            weight=15,\n",
    "            base=0.5)\n",
    "print('...done converting to latex.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52ee2977380704a66854748a73250e0671a9318bd5b3fd45a3df9f851ae61629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
